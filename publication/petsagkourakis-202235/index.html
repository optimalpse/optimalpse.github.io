<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.7d50e350d90912be4a631abd659d6041.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Dr. Ehecatl Antonio del Rio Chanona"><meta name=description content="Chemical process optimization and control are affected by (1) plant-model mismatch, (2) process disturbances, and (3) constraints for safe operation. Reinforcement learning by policy optimization would be a natural way to solve this due to its ability to address stochasticity, plant-model mismatch, and directly account for the effect of future uncertainty and its feedback in a proper closed-loop manner; all without the need of an inner optimization loop. One of the main reasons why reinforcement learning has not been considered for industrial processes (or almost any engineering application) is that it lacks a framework to deal with safety critical constraints. Present algorithms for policy optimization use difficult-to-tune penalty parameters, fail to reliably satisfy state constraints or present guarantees only in expectation. We propose a chance constrained policy optimization (CCPO) algorithm which guarantees the satisfaction of joint chance constraints with a high probability — which is crucial for safety critical tasks. This is achieved by the introduction of constraint tightening (backoffs), which are computed simultaneously with the feedback policy. Backoffs are adjusted with Bayesian optimization using the empirical cumulative distribution function of the probabilistic constraints, and are therefore self-tuned. This results in a general methodology that can be imbued into present policy optimization algorithms to enable them to satisfy joint chance constraints with high probability. We present case studies that analyse the performance of the proposed approach."><link rel=alternate hreflang=en-us href=https://optimalpse.github.io/publication/petsagkourakis-202235/><link rel=canonical href=https://optimalpse.github.io/publication/petsagkourakis-202235/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@https://x.com/OptimlPse"><meta property="twitter:creator" content="@https://x.com/OptimlPse"><meta property="twitter:image" content="https://optimalpse.github.io/media/logo_hu5a76b0abcb7258b41069941c74c2d861_196285_300x300_fit_lanczos_3.png"><meta property="og:site_name" content="OptiML PSE"><meta property="og:url" content="https://optimalpse.github.io/publication/petsagkourakis-202235/"><meta property="og:title" content="Chance Constrained Policy Optimization for Process Control and Optimization | OptiML PSE"><meta property="og:description" content="Chemical process optimization and control are affected by (1) plant-model mismatch, (2) process disturbances, and (3) constraints for safe operation. Reinforcement learning by policy optimization would be a natural way to solve this due to its ability to address stochasticity, plant-model mismatch, and directly account for the effect of future uncertainty and its feedback in a proper closed-loop manner; all without the need of an inner optimization loop. One of the main reasons why reinforcement learning has not been considered for industrial processes (or almost any engineering application) is that it lacks a framework to deal with safety critical constraints. Present algorithms for policy optimization use difficult-to-tune penalty parameters, fail to reliably satisfy state constraints or present guarantees only in expectation. We propose a chance constrained policy optimization (CCPO) algorithm which guarantees the satisfaction of joint chance constraints with a high probability — which is crucial for safety critical tasks. This is achieved by the introduction of constraint tightening (backoffs), which are computed simultaneously with the feedback policy. Backoffs are adjusted with Bayesian optimization using the empirical cumulative distribution function of the probabilistic constraints, and are therefore self-tuned. This results in a general methodology that can be imbued into present policy optimization algorithms to enable them to satisfy joint chance constraints with high probability. We present case studies that analyse the performance of the proposed approach."><meta property="og:image" content="https://optimalpse.github.io/media/logo_hu5a76b0abcb7258b41069941c74c2d861_196285_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-05-17T11:36:44+00:00"><meta property="article:modified_time" content="2023-05-17T12:08:16+01:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://optimalpse.github.io/publication/petsagkourakis-202235/"},"headline":"Chance Constrained Policy Optimization for Process Control and Optimization","datePublished":"2023-05-17T11:36:44Z","dateModified":"2023-05-17T12:08:16+01:00","author":{"@type":"Person","name":"Panagiotis Petsagkourakis"},"publisher":{"@type":"Organization","name":"OptiML PSE","logo":{"@type":"ImageObject","url":"https://optimalpse.github.io/media/logo_hu5a76b0abcb7258b41069941c74c2d861_196285_192x192_fit_lanczos_3.png"}},"description":"Chemical process optimization and control are affected by (1) plant-model mismatch, (2) process disturbances, and (3) constraints for safe operation. Reinforcement learning by policy optimization would be a natural way to solve this due to its ability to address stochasticity, plant-model mismatch, and directly account for the effect of future uncertainty and its feedback in a proper closed-loop manner; all without the need of an inner optimization loop. One of the main reasons why reinforcement learning has not been considered for industrial processes (or almost any engineering application) is that it lacks a framework to deal with safety critical constraints. Present algorithms for policy optimization use difficult-to-tune penalty parameters, fail to reliably satisfy state constraints or present guarantees only in expectation. We propose a chance constrained policy optimization (CCPO) algorithm which guarantees the satisfaction of joint chance constraints with a high probability — which is crucial for safety critical tasks. This is achieved by the introduction of constraint tightening (backoffs), which are computed simultaneously with the feedback policy. Backoffs are adjusted with Bayesian optimization using the empirical cumulative distribution function of the probabilistic constraints, and are therefore self-tuned. This results in a general methodology that can be imbued into present policy optimization algorithms to enable them to satisfy joint chance constraints with high probability. We present case studies that analyse the performance of the proposed approach."}</script><title>OptiML PSE</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=12ddcb866dbcdb7a7c0d2efb68edd506><script src=/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex align-items-center"><a class="navbar-brand d-flex align-items-center" href=/><img src=/media/logo_hu5a76b0abcb7258b41069941c74c2d861_196285_0x70_resize_lanczos_3.png alt="OptiML PSE"><span class=ml-3>OptiML PSE</span></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none align-items-center"><a class="navbar-brand d-flex align-items-center" href=/><img src=/media/logo_hu5a76b0abcb7258b41069941c74c2d861_196285_0x70_resize_lanczos_3.png alt="OptiML PSE"><span class=ml-3>OptiML PSE</span></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class="nav-item dropdown"><a href=/research class="nav-link dropdown-toggle" data-hover=dropdown aria-haspopup=true id=navbarDropdownMenuLink><span>Research</span><span class=caret></span></a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/research/bayesian-optimization><span>Bayesian Optimization</span></a>
<a class=dropdown-item href=/research/data-driven-optimization><span>Data-Driven Optimization</span></a>
<a class=dropdown-item href=/research/supply-chain-optimization><span>Supply Chain Optimization</span></a>
<a class=dropdown-item href=/research/reinforcement-learning><span>Reinforcement Learning</span></a>
<a class=dropdown-item href=/research/statistical-learning><span>Statistical Learning</span></a>
<a class=dropdown-item href=/research/large-language-models><span>Large Language Models</span></a>
<a class=dropdown-item href=/research/hybrid-modelling><span>Hybrid Modelling</span></a>
<a class=dropdown-item href=/research/process-control><span>Process Control</span></a>
<a class=dropdown-item href=/research/deep-learning-in-ce><span>Deep Learning in Chemical Engineering</span></a></div></li><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/learning-resources><span>Learning Resources</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://github.com/OptiMaL-PSE-Lab target=_blank rel=noopener aria-label=github><i class="fab fa-github fa-xl" aria-hidden=true></i></a></li><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://www.youtube.com/@optimlpse5033 target=_blank rel=noopener aria-label=youtube><i class="fab fa-youtube fa-xl" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Chance Constrained Policy Optimization for Process Control and Optimization</h1><div class=article-metadata><div><span><a href=/author/panagiotis-petsagkourakis/>Panagiotis Petsagkourakis</a></span>, <span><a href=/author/ilya-orson-sandoval/>Ilya Orson Sandoval</a></span>, <span><a href=/author/eric-bradford/>Eric Bradford</a></span>, <span><a href=/author/federico-galvanin/>Federico Galvanin</a></span>, <span><a href=/author/dongda-zhang/>Dongda Zhang</a></span>, <span><a href=/author/dr.-ehecatl-antonio-del-rio-chanona/>Dr. Ehecatl Antonio del Rio Chanona</a></span></div><span class=article-date>January 2022</span></div><div class="btn-links mb-3"><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/petsagkourakis-202235/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header" href=https://doi.org/https://doi.org/10.1016/j.jprocont.2022.01.003 target=_blank rel=noopener>DOI
</a><a class="btn btn-outline-primary btn-page-header" href=https://www.sciencedirect.com/science/article/pii/S0959152422000038 target=_blank rel=noopener>URL</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Chemical process optimization and control are affected by (1) plant-model mismatch, (2) process disturbances, and (3) constraints for safe operation. Reinforcement learning by policy optimization would be a natural way to solve this due to its ability to address stochasticity, plant-model mismatch, and directly account for the effect of future uncertainty and its feedback in a proper closed-loop manner; all without the need of an inner optimization loop. One of the main reasons why reinforcement learning has not been considered for industrial processes (or almost any engineering application) is that it lacks a framework to deal with safety critical constraints. Present algorithms for policy optimization use difficult-to-tune penalty parameters, fail to reliably satisfy state constraints or present guarantees only in expectation. We propose a chance constrained policy optimization (CCPO) algorithm which guarantees the satisfaction of joint chance constraints with a high probability — which is crucial for safety critical tasks. This is achieved by the introduction of constraint tightening (backoffs), which are computed simultaneously with the feedback policy. Backoffs are adjusted with Bayesian optimization using the empirical cumulative distribution function of the probabilistic constraints, and are therefore self-tuned. This results in a general methodology that can be imbued into present policy optimization algorithms to enable them to satisfy joint chance constraints with high probability. We present case studies that analyse the performance of the proposed approach.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#2>Journal article</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9"><em>Journal of Process Control</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/policy-search/>Policy search</a>
<a class="badge badge-light" href=/tag/reinforcement-learning/>Reinforcement Learning</a>
<a class="badge badge-light" href=/tag/data-driven-process-control/>Data-driven process control</a>
<a class="badge badge-light" href=/tag/chance-constraints/>Chance constraints</a>
<a class="badge badge-light" href=/tag/bayesian-optimization/>Bayesian Optimization</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fpetsagkourakis-202235%2F&amp;text=Chance+Constrained+Policy+Optimization+for+Process+Control+and+Optimization" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fpetsagkourakis-202235%2F&amp;t=Chance+Constrained+Policy+Optimization+for+Process+Control+and+Optimization" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Chance%20Constrained%20Policy%20Optimization%20for%20Process%20Control%20and%20Optimization&amp;body=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fpetsagkourakis-202235%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fpetsagkourakis-202235%2F&amp;title=Chance+Constrained+Policy+Optimization+for+Process+Control+and+Optimization" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Chance+Constrained+Policy+Optimization+for+Process+Control+and+Optimization%20https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fpetsagkourakis-202235%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fpetsagkourakis-202235%2F&amp;title=Chance+Constrained+Policy+Optimization+for+Process+Control+and+Optimization" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=/author/ilya-orson-sandoval/><img class="avatar mr-3 avatar-circle" src=/author/ilya-orson-sandoval/avatar_hu60aa921bce79549d1adafd305174e43b_165406_270x270_fill_q100_lanczos_center.jpg alt="Ilya Orson Sandoval"></a><div class=media-body><h5 class=card-title><a href=/author/ilya-orson-sandoval/>Ilya Orson Sandoval</a></h5><h6 class=card-subtitle>Knowledge-driven Autonomous Systems - Neural ODEs and Reinforcement Learning</h6><p class=card-text>I am a PhD candidate at Imperial College London, where my research focuses on the intersection of reinforcement learning, differentiable programming and nonlinear optimal control. Curiosity driven, usually by applied mathematics and computer simulations with applications over multiple fields! Previously, I worked in data science and software engineering within the energy and food industries in Mexico. I have a background in theoretical and computational physics.</p><ul class=network-icon aria-hidden=true><li><a href=https://ilyaorson.gitlab.io/blog/ target=_blank rel=noopener><i class="fas fa-laptop"></i></a></li><li><a href=https://twitter.com/IlyaOrson target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.linkedin.com/in/ilyaorson/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://scholar.google.com/citations?user=a4L1PPMAAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/ilyaorson target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><a href=/author/dr.-ehecatl-antonio-del-rio-chanona/><img class="avatar mr-3 avatar-circle" src=/author/dr.-ehecatl-antonio-del-rio-chanona/avatar_hu1ccd021fedd366f805bafde0a573a98e_162360_270x270_fill_q100_lanczos_center.jpg alt="Dr. Ehecatl Antonio del Rio Chanona"></a><div class=media-body><h5 class=card-title><a href=/author/dr.-ehecatl-antonio-del-rio-chanona/>Dr. Ehecatl Antonio del Rio Chanona</a></h5><h6 class=card-subtitle>Principal Investigator of OptiML</h6><p class=card-text>Antonio del Rio Chanona is the head of the Optimisation and Machine Learning for Process Systems Engineering group based in thee Department of Chemical Engineering, as well as the Centre for Process Systems Engineering at Imperial College London. His work is at the forefront of integrating advanced computer algorithms from optimization, machine learning, and reinforcement learning into engineering systems, with a particular focus on bioprocess control, optimization, and scale-up. Dr. del Rio Chanona earned his PhD from the Department of Chemical Engineering and Biotechnology at the University of Cambridge, where his outstanding research earned him the prestigious Danckwerts-Pergamon award for the best PhD dissertation of 2017. He completed his undergraduate studies at the National Autonomous University of Mexico (UNAM), which laid the foundation for his expertise in engineering.</p><ul class=network-icon aria-hidden=true><li><a href="https://twitter.com/antonioe89?lang=en" target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=iitJzzwAAAAJ&amp;hl=es" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/OptiMaL-PSE-Lab target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.youtube.com/@optimlpse5033 target=_blank rel=noopener><i class="fab fa-youtube"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 OptiML PSE. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.4922cd6d3d810ab587afa7cdb3851db6.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.af9327db0521d4a01354bfc8b77a4324.js type=module></script></body></html>