<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d30bac6526e4430eaf089f95e1f804bf.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Dr. Ehecatl Antonio del Rio Chanona"><meta name=description content="Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system."><link rel=alternate hreflang=en-us href=https://optimalpse.github.io/publication/mousa-analysis-2023/><link rel=canonical href=https://optimalpse.github.io/publication/mousa-analysis-2023/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://optimalpse.github.io/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="OptiML PSE"><meta property="og:url" content="https://optimalpse.github.io/publication/mousa-analysis-2023/"><meta property="og:title" content="An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems | OptiML PSE"><meta property="og:description" content="Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system."><meta property="og:image" content="https://optimalpse.github.io/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2024-05-18T18:36:02+00:00"><meta property="article:modified_time" content="2024-05-18T19:36:02+01:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://optimalpse.github.io/publication/mousa-analysis-2023/"},"headline":"An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems","datePublished":"2024-05-18T18:36:02Z","dateModified":"2024-05-18T19:36:02+01:00","author":{"@type":"Person","name":"Marwan Mousa"},"publisher":{"@type":"Organization","name":"OptiML PSE","logo":{"@type":"ImageObject","url":"https://optimalpse.github.io/media/icon_hu5a76b0abcb7258b41069941c74c2d861_196285_192x192_fill_lanczos_center_3.png"}},"description":"Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system."}</script><title>An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems | OptiML PSE</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=92dfe465cbc9e3f48259a4ef157698a8><script src=/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>OptiML PSE</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>OptiML PSE</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Research</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/research/bayesian-optimization><span>Bayesian Optimization</span></a>
<a class=dropdown-item href=/research/data-driven-optimization><span>Data-Driven Optimization</span></a>
<a class=dropdown-item href=/research/supply-chain-optimization><span>Supply Chain Optimization</span></a>
<a class=dropdown-item href=/research/reinforcement-learning><span>Reinforcement Learning</span></a>
<a class=dropdown-item href=/research/statistical-learning><span>Statistical Learning</span></a>
<a class=dropdown-item href=/research/large-language-models><span>Large Language Models</span></a>
<a class=dropdown-item href=/research/hybrid-modelling><span>Hybrid Modelling</span></a>
<a class=dropdown-item href=/research/process-control><span>Process Control</span></a>
<a class=dropdown-item href=/research/deep-learning-in-ce><span>Deep Learning in Chemical Engineering</span></a></div></li><li class=nav-item><a class=nav-link href=/news><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/learning-resources><span>Learning Resources</span></a></li><li class=nav-item><a class=nav-link href=/contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/OptiMaL-PSE-Lab target=_blank rel=noopener><span>GitHub</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems</h1><div class=article-metadata><div><span><a href=/author/marwan-mousa/>Marwan Mousa</a></span>, <span><a href=/author/damin/>damin</a></span>, <span><a href=/author/niki-kotecha/>Niki Kotecha</a></span>, <span><a href=/author/dr.-ehecatl-antonio-del-rio-chanona/>Dr. Ehecatl Antonio del Rio Chanona</a></span>, <span><a href=/author/max-mowbray/>Max Mowbray</a></span></div><span class=article-date>July 2023</span></div><div class="btn-links mb-3"><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/mousa-analysis-2023/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header" href=https://doi.org/10.48550/arXiv.2307.11432 target=_blank rel=noopener>DOI
</a><a class="btn btn-outline-primary btn-page-header" href=http://arxiv.org/abs/2307.11432 target=_blank rel=noopener>URL</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9"><em>arXiv</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tag/computer-science-machine-learning/>Computer Science - Machine Learning</a>
<a class="badge badge-light" href=/tag/computer-science-multiagent-systems/>Computer Science - Multiagent Systems</a>
<a class="badge badge-light" href=/tag/electrical-engineering-and-systems-science-systems-and-control/>Electrical Engineering and Systems Science - Systems and Control</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fmousa-analysis-2023%2F&amp;text=An+Analysis+of+Multi-Agent+Reinforcement+Learning+for+Decentralized+Inventory+Control+Systems" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fmousa-analysis-2023%2F&amp;t=An+Analysis+of+Multi-Agent+Reinforcement+Learning+for+Decentralized+Inventory+Control+Systems" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=An%20Analysis%20of%20Multi-Agent%20Reinforcement%20Learning%20for%20Decentralized%20Inventory%20Control%20Systems&amp;body=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fmousa-analysis-2023%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fmousa-analysis-2023%2F&amp;title=An+Analysis+of+Multi-Agent+Reinforcement+Learning+for+Decentralized+Inventory+Control+Systems" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=An+Analysis+of+Multi-Agent+Reinforcement+Learning+for+Decentralized+Inventory+Control+Systems%20https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fmousa-analysis-2023%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Foptimalpse.github.io%2Fpublication%2Fmousa-analysis-2023%2F&amp;title=An+Analysis+of+Multi-Agent+Reinforcement+Learning+for+Decentralized+Inventory+Control+Systems" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=/author/niki-kotecha/><img class="avatar mr-3 avatar-circle" src=/author/niki-kotecha/avatar_hue20dea202001b48c36aaac7bdde8869b_144626_270x270_fill_q100_lanczos_center.jpg alt="Niki Kotecha"></a><div class=media-body><h5 class=card-title><a href=/author/niki-kotecha/>Niki Kotecha</a></h5><h6 class=card-subtitle>Second Year PhD Candidate</h6><p class=card-text>Niki Kotecha is a PhD Candidate with a research focus on developing reinforcement learning algorithms for inventory control. She is also interested in multi-agent systems applied to supply chain optimisation. Before her PhD, she previously completed her MEng and BA in Chemical Engineering at the University of Cambridge.</p><ul class=network-icon aria-hidden=true><li><a href=https://www.linkedin.com/in/niki-kotecha/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://twitter.com/KotechaNiki target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://github.com/nikikotecha target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><a href=/author/dr.-ehecatl-antonio-del-rio-chanona/><img class="avatar mr-3 avatar-circle" src=/author/dr.-ehecatl-antonio-del-rio-chanona/avatar_hu1ccd021fedd366f805bafde0a573a98e_162360_270x270_fill_q100_lanczos_center.jpg alt="Dr. Ehecatl Antonio del Rio Chanona"></a><div class=media-body><h5 class=card-title><a href=/author/dr.-ehecatl-antonio-del-rio-chanona/>Dr. Ehecatl Antonio del Rio Chanona</a></h5><h6 class=card-subtitle>Principal Investigator of OptiML</h6><p class=card-text>Antonio del Rio Chanona is the head of the Optimisation and Machine Learning for Process Systems Engineering group based in thee Department of Chemical Engineering, as well as the Centre for Process Systems Engineering at Imperial College London. His work is at the forefront of integrating advanced computer algorithms from optimization, machine learning, and reinforcement learning into engineering systems, with a particular focus on bioprocess control, optimization, and scale-up. Dr. del Rio Chanona earned his PhD from the Department of Chemical Engineering and Biotechnology at the University of Cambridge, where his outstanding research earned him the prestigious Danckwerts-Pergamon award for the best PhD dissertation of 2017. He completed his undergraduate studies at the National Autonomous University of Mexico (UNAM), which laid the foundation for his expertise in engineering.</p><ul class=network-icon aria-hidden=true><li><a href="https://twitter.com/antonioe89?lang=en" target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=iitJzzwAAAAJ&amp;hl=es" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 OptiML PSE. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.4922cd6d3d810ab587afa7cdb3851db6.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.af9327db0521d4a01354bfc8b77a4324.js type=module></script></body></html>